{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseNB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ca9dd85b543a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseNB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"\n\u001b[1;32m      3\u001b[0m     \u001b[0mGaussian\u001b[0m \u001b[0mNaive\u001b[0m \u001b[0mBayes\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mGaussianNB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mCan\u001b[0m \u001b[0mperform\u001b[0m \u001b[0monline\u001b[0m \u001b[0mupdates\u001b[0m \u001b[0mto\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0mvia\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mFor\u001b[0m \u001b[0mdetails\u001b[0m \u001b[0mon\u001b[0m \u001b[0malgorithm\u001b[0m \u001b[0mused\u001b[0m \u001b[0mto\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0mmeans\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvariance\u001b[0m \u001b[0monline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseNB' is not defined"
     ]
    }
   ],
   "source": [
    "class GaussianNB(BaseNB):\n",
    "\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "\n",
    "        X, y = check_X_y(X, y)\n",
    "        return self._partial_fit(X, y, np.unique(y), _refit=True,\n",
    "                                 sample_weight=sample_weight)\n",
    "\n",
    "    @staticmethod\n",
    "    def _update_mean_variance(n_past, mu, var, X, sample_weight=None):\n",
    " \n",
    "        if X.shape[0] == 0:\n",
    "            return mu, var\n",
    "\n",
    "        # Compute (potentially weighted) mean and variance of new datapoints\n",
    "        if sample_weight is not None:\n",
    "            n_new = float(sample_weight.sum())\n",
    "            new_mu = np.average(X, axis=0, weights=sample_weight / n_new)\n",
    "            new_var = np.average((X - new_mu) ** 2, axis=0,\n",
    "                                 weights=sample_weight / n_new)\n",
    "        else:\n",
    "            n_new = X.shape[0]\n",
    "            new_var = np.var(X, axis=0)\n",
    "            new_mu = np.mean(X, axis=0)\n",
    "\n",
    "        if n_past == 0:\n",
    "            return new_mu, new_var\n",
    "\n",
    "        n_total = float(n_past + n_new)\n",
    "\n",
    "        # Combine mean of old and new data, taking into consideration\n",
    "        # (weighted) number of observations\n",
    "        total_mu = (n_new * new_mu + n_past * mu) / n_total\n",
    "\n",
    "        # Combine variance of old and new data, taking into consideration\n",
    "        # (weighted) number of observations. This is achieved by combining\n",
    "        # the sum-of-squared-differences (ssd)\n",
    "        old_ssd = n_past * var\n",
    "        new_ssd = n_new * new_var\n",
    "        total_ssd = (old_ssd + new_ssd +\n",
    "                     (n_past / float(n_new * n_total)) *\n",
    "                     (n_new * mu - n_new * new_mu) ** 2)\n",
    "        total_var = total_ssd / n_total\n",
    "\n",
    "        return total_mu, total_var\n",
    "\n",
    "    def partial_fit(self, X, y, classes=None, sample_weight=None):\n",
    "\n",
    "        return self._partial_fit(X, y, classes, _refit=False,\n",
    "                                 sample_weight=sample_weight)\n",
    "\n",
    "    def _partial_fit(self, X, y, classes=None, _refit=False,\n",
    "                     sample_weight=None):\n",
    "\n",
    "        X, y = check_X_y(X, y)\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = check_array(sample_weight, ensure_2d=False)\n",
    "            check_consistent_length(y, sample_weight)\n",
    "\n",
    "        # If the ratio of data variance between dimensions is too small, it\n",
    "        # will cause numerical errors. To address this, we artificially\n",
    "        # boost the variance by epsilon, a small fraction of the standard\n",
    "        # deviation of the largest dimension.\n",
    "        self.epsilon_ = self.var_smoothing * np.var(X, axis=0).max()\n",
    "\n",
    "        if _refit:\n",
    "            self.classes_ = None\n",
    "\n",
    "        if _check_partial_fit_first_call(self, classes):\n",
    "            # This is the first call to partial_fit:\n",
    "            # initialize various cumulative counters\n",
    "            n_features = X.shape[1]\n",
    "            n_classes = len(self.classes_)\n",
    "            self.theta_ = np.zeros((n_classes, n_features))\n",
    "            self.sigma_ = np.zeros((n_classes, n_features))\n",
    "\n",
    "            self.class_count_ = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "            # Initialise the class prior\n",
    "            # Take into account the priors\n",
    "            if self.priors is not None:\n",
    "                priors = np.asarray(self.priors)\n",
    "                # Check that the provide prior match the number of classes\n",
    "                if len(priors) != n_classes:\n",
    "                    raise ValueError('Number of priors must match number of'\n",
    "                                     ' classes.')\n",
    "                # Check that the sum is 1\n",
    "                if not np.isclose(priors.sum(), 1.0):\n",
    "                    raise ValueError('The sum of the priors should be 1.')\n",
    "                # Check that the prior are non-negative\n",
    "                if (priors < 0).any():\n",
    "                    raise ValueError('Priors must be non-negative.')\n",
    "                self.class_prior_ = priors\n",
    "            else:\n",
    "                # Initialize the priors to zeros for each class\n",
    "                self.class_prior_ = np.zeros(len(self.classes_),\n",
    "                                             dtype=np.float64)\n",
    "        else:\n",
    "            if X.shape[1] != self.theta_.shape[1]:\n",
    "                msg = \"Number of features %d does not match previous data %d.\"\n",
    "                raise ValueError(msg % (X.shape[1], self.theta_.shape[1]))\n",
    "            # Put epsilon back in each time\n",
    "            self.sigma_[:, :] -= self.epsilon_\n",
    "\n",
    "        classes = self.classes_\n",
    "\n",
    "        unique_y = np.unique(y)\n",
    "        unique_y_in_classes = np.in1d(unique_y, classes)\n",
    "\n",
    "        if not np.all(unique_y_in_classes):\n",
    "            raise ValueError(\"The target label(s) %s in y do not exist in the \"\n",
    "                             \"initial classes %s\" %\n",
    "                             (unique_y[~unique_y_in_classes], classes))\n",
    "\n",
    "        for y_i in unique_y:\n",
    "            i = classes.searchsorted(y_i)\n",
    "            X_i = X[y == y_i, :]\n",
    "\n",
    "            if sample_weight is not None:\n",
    "                sw_i = sample_weight[y == y_i]\n",
    "                N_i = sw_i.sum()\n",
    "            else:\n",
    "                sw_i = None\n",
    "                N_i = X_i.shape[0]\n",
    "\n",
    "            new_theta, new_sigma = self._update_mean_variance(\n",
    "                self.class_count_[i], self.theta_[i, :], self.sigma_[i, :],\n",
    "                X_i, sw_i)\n",
    "\n",
    "            self.theta_[i, :] = new_theta\n",
    "            self.sigma_[i, :] = new_sigma\n",
    "            self.class_count_[i] += N_i\n",
    "\n",
    "        self.sigma_[:, :] += self.epsilon_\n",
    "\n",
    "        # Update if only no priors is provided\n",
    "        if self.priors is None:\n",
    "            # Empirical prior, with sample_weight taken into account\n",
    "            self.class_prior_ = self.class_count_ / self.class_count_.sum()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _joint_log_likelihood(self, X):\n",
    "        check_is_fitted(self, \"classes_\")\n",
    "\n",
    "        X = check_array(X)\n",
    "        joint_log_likelihood = []\n",
    "        for i in range(np.size(self.classes_)):\n",
    "            jointi = np.log(self.class_prior_[i])\n",
    "            n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))\n",
    "            n_ij -= 0.5 * np.sum(((X - self.theta_[i, :]) ** 2) /\n",
    "                                 (self.sigma_[i, :]), 1)\n",
    "            joint_log_likelihood.append(jointi + n_ij)\n",
    "\n",
    "        joint_log_likelihood = np.array(joint_log_likelihood).T\n",
    "        return joint_log_likelihood\n",
    "\n",
    "_ALPHA_MIN = 1e-10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNB(BaseNB):\n",
    "    \"\"\"\n",
    "    Gaussian Naive Bayes (GaussianNB)\n",
    "    Can perform online updates to model parameters via `partial_fit` method.\n",
    "    For details on algorithm used to update feature means and variance online,\n",
    "    see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\n",
    "        http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n",
    "    Read more in the :ref:`User Guide <gaussian_naive_bayes>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    priors : array-like, shape (n_classes,)\n",
    "        Prior probabilities of the classes. If specified the priors are not\n",
    "        adjusted according to the data.\n",
    "    var_smoothing : float, optional (default=1e-9)\n",
    "        Portion of the largest variance of all features that is added to\n",
    "        variances for calculation stability.\n",
    "    Attributes\n",
    "    ----------\n",
    "    class_prior_ : array, shape (n_classes,)\n",
    "        probability of each class.\n",
    "    class_count_ : array, shape (n_classes,)\n",
    "        number of training samples observed in each class.\n",
    "    theta_ : array, shape (n_classes, n_features)\n",
    "        mean of each feature per class\n",
    "    sigma_ : array, shape (n_classes, n_features)\n",
    "        variance of each feature per class\n",
    "    epsilon_ : float\n",
    "        absolute additive value to variances\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "    >>> Y = np.array([1, 1, 1, 2, 2, 2])\n",
    "    >>> from sklearn.naive_bayes import GaussianNB\n",
    "    >>> clf = GaussianNB()\n",
    "    >>> clf.fit(X, Y)\n",
    "    GaussianNB(priors=None, var_smoothing=1e-09)\n",
    "    >>> print(clf.predict([[-0.8, -1]]))\n",
    "    [1]\n",
    "    >>> clf_pf = GaussianNB()\n",
    "    >>> clf_pf.partial_fit(X, Y, np.unique(Y))\n",
    "    GaussianNB(priors=None, var_smoothing=1e-09)\n",
    "    >>> print(clf_pf.predict([[-0.8, -1]]))\n",
    "    [1]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, priors=None, var_smoothing=1e-9):\n",
    "        self.priors = priors\n",
    "        self.var_smoothing = var_smoothing\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"Fit Gaussian Naive Bayes according to X, y\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training vectors, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target values.\n",
    "        sample_weight : array-like, shape (n_samples,), optional (default=None)\n",
    "            Weights applied to individual samples (1. for unweighted).\n",
    "            .. versionadded:: 0.17\n",
    "               Gaussian Naive Bayes supports fitting with *sample_weight*.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        X, y = check_X_y(X, y)\n",
    "        return self._partial_fit(X, y, np.unique(y), _refit=True,\n",
    "                                 sample_weight=sample_weight)\n",
    "\n",
    "    @staticmethod\n",
    "    def _update_mean_variance(n_past, mu, var, X, sample_weight=None):\n",
    "        \"\"\"Compute online update of Gaussian mean and variance.\n",
    "        Given starting sample count, mean, and variance, a new set of\n",
    "        points X, and optionally sample weights, return the updated mean and\n",
    "        variance. (NB - each dimension (column) in X is treated as independent\n",
    "        -- you get variance, not covariance).\n",
    "        Can take scalar mean and variance, or vector mean and variance to\n",
    "        simultaneously update a number of independent Gaussians.\n",
    "        See Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\n",
    "        http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_past : int\n",
    "            Number of samples represented in old mean and variance. If sample\n",
    "            weights were given, this should contain the sum of sample\n",
    "            weights represented in old mean and variance.\n",
    "        mu : array-like, shape (number of Gaussians,)\n",
    "            Means for Gaussians in original set.\n",
    "        var : array-like, shape (number of Gaussians,)\n",
    "            Variances for Gaussians in original set.\n",
    "        sample_weight : array-like, shape (n_samples,), optional (default=None)\n",
    "            Weights applied to individual samples (1. for unweighted).\n",
    "        Returns\n",
    "        -------\n",
    "        total_mu : array-like, shape (number of Gaussians,)\n",
    "            Updated mean for each Gaussian over the combined set.\n",
    "        total_var : array-like, shape (number of Gaussians,)\n",
    "            Updated variance for each Gaussian over the combined set.\n",
    "        \"\"\"\n",
    "        if X.shape[0] == 0:\n",
    "            return mu, var\n",
    "\n",
    "        # Compute (potentially weighted) mean and variance of new datapoints\n",
    "        if sample_weight is not None:\n",
    "            n_new = float(sample_weight.sum())\n",
    "            new_mu = np.average(X, axis=0, weights=sample_weight / n_new)\n",
    "            new_var = np.average((X - new_mu) ** 2, axis=0,\n",
    "                                 weights=sample_weight / n_new)\n",
    "        else:\n",
    "            n_new = X.shape[0]\n",
    "            new_var = np.var(X, axis=0)\n",
    "            new_mu = np.mean(X, axis=0)\n",
    "\n",
    "        if n_past == 0:\n",
    "            return new_mu, new_var\n",
    "\n",
    "        n_total = float(n_past + n_new)\n",
    "\n",
    "        # Combine mean of old and new data, taking into consideration\n",
    "        # (weighted) number of observations\n",
    "        total_mu = (n_new * new_mu + n_past * mu) / n_total\n",
    "\n",
    "        # Combine variance of old and new data, taking into consideration\n",
    "        # (weighted) number of observations. This is achieved by combining\n",
    "        # the sum-of-squared-differences (ssd)\n",
    "        old_ssd = n_past * var\n",
    "        new_ssd = n_new * new_var\n",
    "        total_ssd = (old_ssd + new_ssd +\n",
    "                     (n_past / float(n_new * n_total)) *\n",
    "                     (n_new * mu - n_new * new_mu) ** 2)\n",
    "        total_var = total_ssd / n_total\n",
    "\n",
    "        return total_mu, total_var\n",
    "\n",
    "    def partial_fit(self, X, y, classes=None, sample_weight=None):\n",
    "        \"\"\"Incremental fit on a batch of samples.\n",
    "        This method is expected to be called several times consecutively\n",
    "        on different chunks of a dataset so as to implement out-of-core\n",
    "        or online learning.\n",
    "        This is especially useful when the whole dataset is too big to fit in\n",
    "        memory at once.\n",
    "        This method has some performance and numerical stability overhead,\n",
    "        hence it is better to call partial_fit on chunks of data that are\n",
    "        as large as possible (as long as fitting in the memory budget) to\n",
    "        hide the overhead.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target values.\n",
    "        classes : array-like, shape (n_classes,), optional (default=None)\n",
    "            List of all the classes that can possibly appear in the y vector.\n",
    "            Must be provided at the first call to partial_fit, can be omitted\n",
    "            in subsequent calls.\n",
    "        sample_weight : array-like, shape (n_samples,), optional (default=None)\n",
    "            Weights applied to individual samples (1. for unweighted).\n",
    "            .. versionadded:: 0.17\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        return self._partial_fit(X, y, classes, _refit=False,\n",
    "                                 sample_weight=sample_weight)\n",
    "\n",
    "    def _partial_fit(self, X, y, classes=None, _refit=False,\n",
    "                     sample_weight=None):\n",
    "        \"\"\"Actual implementation of Gaussian NB fitting.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target values.\n",
    "        classes : array-like, shape (n_classes,), optional (default=None)\n",
    "            List of all the classes that can possibly appear in the y vector.\n",
    "            Must be provided at the first call to partial_fit, can be omitted\n",
    "            in subsequent calls.\n",
    "        _refit : bool, optional (default=False)\n",
    "            If true, act as though this were the first time we called\n",
    "            _partial_fit (ie, throw away any past fitting and start over).\n",
    "        sample_weight : array-like, shape (n_samples,), optional (default=None)\n",
    "            Weights applied to individual samples (1. for unweighted).\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        X, y = check_X_y(X, y)\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = check_array(sample_weight, ensure_2d=False)\n",
    "            check_consistent_length(y, sample_weight)\n",
    "\n",
    "        # If the ratio of data variance between dimensions is too small, it\n",
    "        # will cause numerical errors. To address this, we artificially\n",
    "        # boost the variance by epsilon, a small fraction of the standard\n",
    "        # deviation of the largest dimension.\n",
    "        self.epsilon_ = self.var_smoothing * np.var(X, axis=0).max()\n",
    "\n",
    "        if _refit:\n",
    "            self.classes_ = None\n",
    "\n",
    "        if _check_partial_fit_first_call(self, classes):\n",
    "            # This is the first call to partial_fit:\n",
    "            # initialize various cumulative counters\n",
    "            n_features = X.shape[1]\n",
    "            n_classes = len(self.classes_)\n",
    "            self.theta_ = np.zeros((n_classes, n_features))\n",
    "            self.sigma_ = np.zeros((n_classes, n_features))\n",
    "\n",
    "            self.class_count_ = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "            # Initialise the class prior\n",
    "            # Take into account the priors\n",
    "            if self.priors is not None:\n",
    "                priors = np.asarray(self.priors)\n",
    "                # Check that the provide prior match the number of classes\n",
    "                if len(priors) != n_classes:\n",
    "                    raise ValueError('Number of priors must match number of'\n",
    "                                     ' classes.')\n",
    "                # Check that the sum is 1\n",
    "                if not np.isclose(priors.sum(), 1.0):\n",
    "                    raise ValueError('The sum of the priors should be 1.')\n",
    "                # Check that the prior are non-negative\n",
    "                if (priors < 0).any():\n",
    "                    raise ValueError('Priors must be non-negative.')\n",
    "                self.class_prior_ = priors\n",
    "            else:\n",
    "                # Initialize the priors to zeros for each class\n",
    "                self.class_prior_ = np.zeros(len(self.classes_),\n",
    "                                             dtype=np.float64)\n",
    "        else:\n",
    "            if X.shape[1] != self.theta_.shape[1]:\n",
    "                msg = \"Number of features %d does not match previous data %d.\"\n",
    "                raise ValueError(msg % (X.shape[1], self.theta_.shape[1]))\n",
    "            # Put epsilon back in each time\n",
    "            self.sigma_[:, :] -= self.epsilon_\n",
    "\n",
    "        classes = self.classes_\n",
    "\n",
    "        unique_y = np.unique(y)\n",
    "        unique_y_in_classes = np.in1d(unique_y, classes)\n",
    "\n",
    "        if not np.all(unique_y_in_classes):\n",
    "            raise ValueError(\"The target label(s) %s in y do not exist in the \"\n",
    "                             \"initial classes %s\" %\n",
    "                             (unique_y[~unique_y_in_classes], classes))\n",
    "\n",
    "        for y_i in unique_y:\n",
    "            i = classes.searchsorted(y_i)\n",
    "            X_i = X[y == y_i, :]\n",
    "\n",
    "            if sample_weight is not None:\n",
    "                sw_i = sample_weight[y == y_i]\n",
    "                N_i = sw_i.sum()\n",
    "            else:\n",
    "                sw_i = None\n",
    "                N_i = X_i.shape[0]\n",
    "\n",
    "            new_theta, new_sigma = self._update_mean_variance(\n",
    "                self.class_count_[i], self.theta_[i, :], self.sigma_[i, :],\n",
    "                X_i, sw_i)\n",
    "\n",
    "            self.theta_[i, :] = new_theta\n",
    "            self.sigma_[i, :] = new_sigma\n",
    "            self.class_count_[i] += N_i\n",
    "\n",
    "        self.sigma_[:, :] += self.epsilon_\n",
    "\n",
    "        # Update if only no priors is provided\n",
    "        if self.priors is None:\n",
    "            # Empirical prior, with sample_weight taken into account\n",
    "            self.class_prior_ = self.class_count_ / self.class_count_.sum()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _joint_log_likelihood(self, X):\n",
    "        check_is_fitted(self, \"classes_\")\n",
    "\n",
    "        X = check_array(X)\n",
    "        joint_log_likelihood = []\n",
    "        for i in range(np.size(self.classes_)):\n",
    "            jointi = np.log(self.class_prior_[i])\n",
    "            n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))\n",
    "            n_ij -= 0.5 * np.sum(((X - self.theta_[i, :]) ** 2) /\n",
    "                                 (self.sigma_[i, :]), 1)\n",
    "            joint_log_likelihood.append(jointi + n_ij)\n",
    "\n",
    "        joint_log_likelihood = np.array(joint_log_likelihood).T\n",
    "        return joint_log_likelihood\n",
    "\n",
    "_ALPHA_MIN = 1e-10\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
